
==> Audit <==
|---------|--------------------------------|----------|----------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |   User   | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|----------|---------|---------------------|---------------------|
| start   |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 11:23 CST | 18 Sep 24 11:23 CST |
| start   | --cpus 4 --memory 10240        | minikube | benjamin | v1.34.0 | 18 Sep 24 12:57 CST | 18 Sep 24 12:58 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:00 CST | 18 Sep 24 13:00 CST |
| start   | --cpus 6 --memory 20480        | minikube | benjamin | v1.34.0 | 18 Sep 24 13:01 CST |                     |
| start   | --cpus 6 --memory 10240        | minikube | benjamin | v1.34.0 | 18 Sep 24 13:02 CST | 18 Sep 24 13:02 CST |
| delete  |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:04 CST | 18 Sep 24 13:04 CST |
| start   | --cpus 6 --memory 10240        | minikube | benjamin | v1.34.0 | 18 Sep 24 13:05 CST | 18 Sep 24 13:05 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:08 CST | 18 Sep 24 13:08 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:22 CST | 18 Sep 24 13:22 CST |
| delete  |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:22 CST | 18 Sep 24 13:22 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:38 CST | 18 Sep 24 13:38 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 18 Sep 24 13:45 CST | 18 Sep 24 14:30 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:55 CST | 18 Sep 24 13:55 CST |
| delete  |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 13:56 CST | 18 Sep 24 13:56 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 14:13 CST | 18 Sep 24 14:13 CST |
| service | jenkins -n jenkis              | minikube | benjamin | v1.34.0 | 18 Sep 24 14:17 CST |                     |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 18 Sep 24 14:18 CST |                     |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 18 Sep 24 14:24 CST |                     |
| stop    |                                | minikube | benjamin | v1.34.0 | 18 Sep 24 17:18 CST | 18 Sep 24 17:19 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 19 Sep 24 12:27 CST | 19 Sep 24 12:27 CST |
| service | pruebadc-deployment --url      | minikube | benjamin | v1.34.0 | 19 Sep 24 12:33 CST | 19 Sep 24 12:33 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 11:39 CST | 20 Sep 24 11:39 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 20 Sep 24 12:19 CST | 20 Sep 24 14:30 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 14:28 CST | 20 Sep 24 14:28 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 14:28 CST | 20 Sep 24 14:29 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 20 Sep 24 14:30 CST | 20 Sep 24 14:31 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 14:31 CST | 20 Sep 24 14:31 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 14:32 CST | 20 Sep 24 14:32 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 20 Sep 24 14:32 CST | 20 Sep 24 14:32 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 20 Sep 24 14:35 CST | 21 Sep 24 09:21 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 19:15 CST | 20 Sep 24 19:15 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 19:16 CST | 20 Sep 24 19:16 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 19:17 CST | 20 Sep 24 19:17 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 19:41 CST | 20 Sep 24 19:41 CST |
| addons  | enable ingress                 | minikube | benjamin | v1.34.0 | 20 Sep 24 19:43 CST | 20 Sep 24 19:47 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 20 Sep 24 21:41 CST | 20 Sep 24 21:41 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 21 Sep 24 09:20 CST | 21 Sep 24 09:20 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 23 Sep 24 09:18 CST | 23 Sep 24 09:18 CST |
| service | jenkins --namespace=jenkins    | minikube | benjamin | v1.34.0 | 23 Sep 24 09:23 CST | 24 Sep 24 09:47 CST |
| service | jenkins --namespace=jenkins    | minikube | benjamin | v1.34.0 | 23 Sep 24 09:30 CST | 23 Sep 24 09:30 CST |
| service | jenkins --namespace=gitlab     | minikube | benjamin | v1.34.0 | 23 Sep 24 09:30 CST |                     |
| service | gitlab --namespace=gitlab      | minikube | benjamin | v1.34.0 | 23 Sep 24 09:32 CST |                     |
| service | gitlab-webservice-default      | minikube | benjamin | v1.34.0 | 23 Sep 24 09:35 CST | 23 Sep 24 10:46 CST |
|         | --namespace=gitlab             |          |          |         |                     |                     |
| service | gitlab-webservice-default      | minikube | benjamin | v1.34.0 | 23 Sep 24 10:47 CST |                     |
|         | --namespace=gitlab             |          |          |         |                     |                     |
| stop    |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 10:46 CST | 24 Sep 24 10:46 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 10:48 CST | 24 Sep 24 10:48 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 11:16 CST | 24 Sep 24 11:16 CST |
| delete  |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 11:17 CST | 24 Sep 24 11:17 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 11:18 CST | 24 Sep 24 11:19 CST |
| ip      |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 13:11 CST | 24 Sep 24 13:11 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 24 Sep 24 17:14 CST | 24 Sep 24 17:14 CST |
| start   | --cpus=6 --memory=10240        | minikube | benjamin | v1.34.0 | 24 Sep 24 17:14 CST | 24 Sep 24 17:15 CST |
| start   |                                | minikube | benjamin | v1.34.0 | 25 Sep 24 10:23 CST | 25 Sep 24 10:23 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 25 Sep 24 10:37 CST | 25 Sep 24 10:37 CST |
| service | jenkins -n jenkins             | minikube | benjamin | v1.34.0 | 25 Sep 24 10:42 CST | 25 Sep 24 10:42 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 25 Sep 24 10:51 CST | 25 Sep 24 10:51 CST |
| start   | --cpus=6 --memory=10240        | minikube | benjamin | v1.34.0 | 25 Sep 24 10:51 CST | 25 Sep 24 10:51 CST |
| stop    |                                | minikube | benjamin | v1.34.0 | 25 Sep 24 10:55 CST | 25 Sep 24 10:55 CST |
| start   | --cpus=6 --memory=8192         | minikube | benjamin | v1.34.0 | 25 Sep 24 10:56 CST | 25 Sep 24 10:56 CST |
| service | nginx --url                    | minikube | benjamin | v1.34.0 | 25 Sep 24 12:19 CST |                     |
|---------|--------------------------------|----------|----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/09/25 10:56:16
Running on machine: jameto
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0925 10:56:16.221055   32346 out.go:345] Setting OutFile to fd 1 ...
I0925 10:56:16.221214   32346 out.go:397] isatty.IsTerminal(1) = true
I0925 10:56:16.221222   32346 out.go:358] Setting ErrFile to fd 2...
I0925 10:56:16.221231   32346 out.go:397] isatty.IsTerminal(2) = true
I0925 10:56:16.221493   32346 root.go:338] Updating PATH: /home/benjamin/.minikube/bin
I0925 10:56:16.221938   32346 out.go:352] Setting JSON to false
I0925 10:56:16.238108   32346 start.go:129] hostinfo: {"hostname":"jameto","uptime":1994,"bootTime":1727281382,"procs":334,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"12.7","kernelVersion":"6.1.0-25-amd64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"be167fef-75dc-424a-9636-07546838ee23"}
I0925 10:56:16.238186   32346 start.go:139] virtualization: kvm host
I0925 10:56:16.249180   32346 out.go:177] 😄  minikube v1.34.0 en Debian 12.7
I0925 10:56:16.250610   32346 notify.go:220] Checking for updates...
I0925 10:56:16.251150   32346 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0925 10:56:16.251291   32346 driver.go:394] Setting default libvirt URI to qemu:///system
I0925 10:56:16.277466   32346 docker.go:123] docker version: linux-27.3.0:Docker Engine - Community
I0925 10:56:16.277602   32346 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0925 10:56:16.326961   32346 info.go:266] docker info: {ID:c7b8cd45-c575-4c8d-a2e7-64fb1cadaccf Containers:12 ContainersRunning:0 ContainersPaused:0 ContainersStopped:12 Images:21 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:40 SystemTime:2024-09-25 10:56:16.316368892 -0600 CST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-25-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:14491123712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:jameto Labels:[] ExperimentalBuild:false ServerVersion:27.3.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.6]] Warnings:<nil>}}
I0925 10:56:16.327100   32346 docker.go:318] overlay module found
I0925 10:56:16.328791   32346 out.go:177] ✨  Using the docker driver based on existing profile
I0925 10:56:16.329951   32346 start.go:297] selected driver: docker
I0925 10:56:16.329966   32346 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8192 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/benjamin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0925 10:56:16.330116   32346 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0925 10:56:16.331352   32346 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0925 10:56:16.381970   32346 info.go:266] docker info: {ID:c7b8cd45-c575-4c8d-a2e7-64fb1cadaccf Containers:12 ContainersRunning:0 ContainersPaused:0 ContainersStopped:12 Images:21 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:40 SystemTime:2024-09-25 10:56:16.370069129 -0600 CST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-25-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:14491123712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:jameto Labels:[] ExperimentalBuild:false ServerVersion:27.3.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.6]] Warnings:<nil>}}
W0925 10:56:16.382386   32346 out.go:270] ❗  You cannot change the CPUs for an existing minikube cluster. Please first delete the cluster.
I0925 10:56:16.382470   32346 cni.go:84] Creating CNI manager for ""
I0925 10:56:16.382492   32346 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0925 10:56:16.382563   32346 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8192 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/benjamin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0925 10:56:16.384659   32346 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0925 10:56:16.385650   32346 cache.go:121] Beginning downloading kic base image for docker with docker
I0925 10:56:16.386712   32346 out.go:177] 🚜  Pulling base image v0.0.45 ...
I0925 10:56:16.387718   32346 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0925 10:56:16.387750   32346 preload.go:146] Found local preload: /home/benjamin/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0925 10:56:16.387757   32346 cache.go:56] Caching tarball of preloaded images
I0925 10:56:16.387771   32346 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0925 10:56:16.387846   32346 preload.go:172] Found /home/benjamin/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0925 10:56:16.387857   32346 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0925 10:56:16.387928   32346 profile.go:143] Saving config to /home/benjamin/.minikube/profiles/minikube/config.json ...
W0925 10:56:16.416556   32346 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0925 10:56:16.416580   32346 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0925 10:56:16.416750   32346 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0925 10:56:16.416772   32346 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0925 10:56:16.416780   32346 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0925 10:56:16.416798   32346 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0925 10:56:16.416808   32346 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0925 10:56:16.815454   32346 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0925 10:56:16.815491   32346 cache.go:194] Successfully downloaded all kic artifacts
I0925 10:56:16.815535   32346 start.go:360] acquireMachinesLock for minikube: {Name:mkf260e5aeb9fecf248d44afa47ba62febd47cba Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0925 10:56:16.815644   32346 start.go:364] duration metric: took 76.683µs to acquireMachinesLock for "minikube"
I0925 10:56:16.815673   32346 start.go:96] Skipping create...Using existing machine configuration
I0925 10:56:16.815702   32346 fix.go:54] fixHost starting: 
I0925 10:56:16.816073   32346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0925 10:56:16.837370   32346 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0925 10:56:16.837405   32346 fix.go:138] unexpected machine state, will restart: <nil>
I0925 10:56:16.839100   32346 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0925 10:56:16.840260   32346 cli_runner.go:164] Run: docker start minikube
I0925 10:56:17.196927   32346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0925 10:56:17.215400   32346 kic.go:430] container "minikube" state is running.
I0925 10:56:17.215846   32346 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0925 10:56:17.234528   32346 profile.go:143] Saving config to /home/benjamin/.minikube/profiles/minikube/config.json ...
I0925 10:56:17.234913   32346 machine.go:93] provisionDockerMachine start ...
I0925 10:56:17.235013   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:17.253030   32346 main.go:141] libmachine: Using SSH client type: native
I0925 10:56:17.253267   32346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0925 10:56:17.253277   32346 main.go:141] libmachine: About to run SSH command:
hostname
I0925 10:56:17.254174   32346 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:53952->127.0.0.1:32778: read: connection reset by peer
I0925 10:56:20.387488   32346 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0925 10:56:20.387515   32346 ubuntu.go:169] provisioning hostname "minikube"
I0925 10:56:20.387636   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:20.406212   32346 main.go:141] libmachine: Using SSH client type: native
I0925 10:56:20.406376   32346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0925 10:56:20.406385   32346 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0925 10:56:20.562321   32346 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0925 10:56:20.562405   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:20.582498   32346 main.go:141] libmachine: Using SSH client type: native
I0925 10:56:20.582776   32346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0925 10:56:20.582802   32346 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0925 10:56:20.721463   32346 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0925 10:56:20.721532   32346 ubuntu.go:175] set auth options {CertDir:/home/benjamin/.minikube CaCertPath:/home/benjamin/.minikube/certs/ca.pem CaPrivateKeyPath:/home/benjamin/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/benjamin/.minikube/machines/server.pem ServerKeyPath:/home/benjamin/.minikube/machines/server-key.pem ClientKeyPath:/home/benjamin/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/benjamin/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/benjamin/.minikube}
I0925 10:56:20.721581   32346 ubuntu.go:177] setting up certificates
I0925 10:56:20.721610   32346 provision.go:84] configureAuth start
I0925 10:56:20.721728   32346 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0925 10:56:20.740408   32346 provision.go:143] copyHostCerts
I0925 10:56:20.740479   32346 exec_runner.go:144] found /home/benjamin/.minikube/ca.pem, removing ...
I0925 10:56:20.740491   32346 exec_runner.go:203] rm: /home/benjamin/.minikube/ca.pem
I0925 10:56:20.740595   32346 exec_runner.go:151] cp: /home/benjamin/.minikube/certs/ca.pem --> /home/benjamin/.minikube/ca.pem (1082 bytes)
I0925 10:56:20.740710   32346 exec_runner.go:144] found /home/benjamin/.minikube/cert.pem, removing ...
I0925 10:56:20.740715   32346 exec_runner.go:203] rm: /home/benjamin/.minikube/cert.pem
I0925 10:56:20.740747   32346 exec_runner.go:151] cp: /home/benjamin/.minikube/certs/cert.pem --> /home/benjamin/.minikube/cert.pem (1127 bytes)
I0925 10:56:20.740799   32346 exec_runner.go:144] found /home/benjamin/.minikube/key.pem, removing ...
I0925 10:56:20.740804   32346 exec_runner.go:203] rm: /home/benjamin/.minikube/key.pem
I0925 10:56:20.740834   32346 exec_runner.go:151] cp: /home/benjamin/.minikube/certs/key.pem --> /home/benjamin/.minikube/key.pem (1675 bytes)
I0925 10:56:20.740884   32346 provision.go:117] generating server cert: /home/benjamin/.minikube/machines/server.pem ca-key=/home/benjamin/.minikube/certs/ca.pem private-key=/home/benjamin/.minikube/certs/ca-key.pem org=benjamin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0925 10:56:20.990542   32346 provision.go:177] copyRemoteCerts
I0925 10:56:20.990606   32346 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0925 10:56:20.990660   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.013336   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:21.115025   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0925 10:56:21.153916   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0925 10:56:21.192124   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0925 10:56:21.227733   32346 provision.go:87] duration metric: took 506.096748ms to configureAuth
I0925 10:56:21.227763   32346 ubuntu.go:193] setting minikube options for container-runtime
I0925 10:56:21.228006   32346 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0925 10:56:21.228298   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.247390   32346 main.go:141] libmachine: Using SSH client type: native
I0925 10:56:21.247559   32346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0925 10:56:21.247567   32346 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0925 10:56:21.389977   32346 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0925 10:56:21.389994   32346 ubuntu.go:71] root file system type: overlay
I0925 10:56:21.390106   32346 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0925 10:56:21.390186   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.408829   32346 main.go:141] libmachine: Using SSH client type: native
I0925 10:56:21.408990   32346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0925 10:56:21.409038   32346 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0925 10:56:21.560660   32346 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0925 10:56:21.560756   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.579140   32346 main.go:141] libmachine: Using SSH client type: native
I0925 10:56:21.579380   32346 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0925 10:56:21.579404   32346 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0925 10:56:21.722861   32346 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0925 10:56:21.722889   32346 machine.go:96] duration metric: took 4.487958942s to provisionDockerMachine
I0925 10:56:21.722905   32346 start.go:293] postStartSetup for "minikube" (driver="docker")
I0925 10:56:21.722926   32346 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0925 10:56:21.723001   32346 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0925 10:56:21.723075   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.741782   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:21.848460   32346 ssh_runner.go:195] Run: cat /etc/os-release
I0925 10:56:21.853709   32346 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0925 10:56:21.853756   32346 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0925 10:56:21.853775   32346 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0925 10:56:21.853791   32346 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0925 10:56:21.853813   32346 filesync.go:126] Scanning /home/benjamin/.minikube/addons for local assets ...
I0925 10:56:21.853921   32346 filesync.go:126] Scanning /home/benjamin/.minikube/files for local assets ...
I0925 10:56:21.853968   32346 start.go:296] duration metric: took 131.048164ms for postStartSetup
I0925 10:56:21.854088   32346 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0925 10:56:21.854193   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.874506   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:21.969540   32346 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0925 10:56:21.976472   32346 fix.go:56] duration metric: took 5.160773963s for fixHost
I0925 10:56:21.976494   32346 start.go:83] releasing machines lock for "minikube", held for 5.16083563s
I0925 10:56:21.976571   32346 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0925 10:56:21.996139   32346 ssh_runner.go:195] Run: cat /version.json
I0925 10:56:21.996218   32346 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0925 10:56:21.996262   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:21.996357   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:22.015567   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:22.015936   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:22.412465   32346 ssh_runner.go:195] Run: systemctl --version
I0925 10:56:22.420530   32346 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0925 10:56:22.427930   32346 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0925 10:56:22.455114   32346 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0925 10:56:22.455229   32346 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0925 10:56:22.468114   32346 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0925 10:56:22.468137   32346 start.go:495] detecting cgroup driver to use...
I0925 10:56:22.468185   32346 detect.go:190] detected "systemd" cgroup driver on host os
I0925 10:56:22.468308   32346 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0925 10:56:22.495862   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0925 10:56:22.513532   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0925 10:56:22.530062   32346 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0925 10:56:22.530178   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0925 10:56:22.544872   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0925 10:56:22.561404   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0925 10:56:22.577651   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0925 10:56:22.594969   32346 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0925 10:56:22.609814   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0925 10:56:22.625882   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0925 10:56:22.639984   32346 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0925 10:56:22.656580   32346 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0925 10:56:22.668403   32346 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0925 10:56:22.680133   32346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0925 10:56:22.768165   32346 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0925 10:56:22.916691   32346 start.go:495] detecting cgroup driver to use...
I0925 10:56:22.916767   32346 detect.go:190] detected "systemd" cgroup driver on host os
I0925 10:56:22.916858   32346 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0925 10:56:22.933816   32346 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0925 10:56:22.933911   32346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0925 10:56:22.950549   32346 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0925 10:56:22.973342   32346 ssh_runner.go:195] Run: which cri-dockerd
I0925 10:56:22.977695   32346 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0925 10:56:22.989122   32346 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0925 10:56:23.016815   32346 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0925 10:56:23.108305   32346 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0925 10:56:23.186595   32346 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0925 10:56:23.186858   32346 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0925 10:56:23.216111   32346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0925 10:56:23.309624   32346 ssh_runner.go:195] Run: sudo systemctl restart docker
I0925 10:56:26.156791   32346 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.847118298s)
I0925 10:56:26.156907   32346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0925 10:56:26.176500   32346 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0925 10:56:26.194523   32346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0925 10:56:26.213047   32346 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0925 10:56:26.285588   32346 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0925 10:56:26.357052   32346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0925 10:56:26.447909   32346 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0925 10:56:26.503353   32346 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0925 10:56:26.520996   32346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0925 10:56:26.621414   32346 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0925 10:56:26.752348   32346 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0925 10:56:26.752470   32346 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0925 10:56:26.757143   32346 start.go:563] Will wait 60s for crictl version
I0925 10:56:26.757218   32346 ssh_runner.go:195] Run: which crictl
I0925 10:56:26.761936   32346 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0925 10:56:26.800235   32346 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0925 10:56:26.800338   32346 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0925 10:56:26.832670   32346 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0925 10:56:26.863795   32346 out.go:235] 🐳  Preparando Kubernetes v1.31.0 en Docker 27.2.0...
I0925 10:56:26.863912   32346 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0925 10:56:26.882833   32346 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0925 10:56:26.888548   32346 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0925 10:56:26.906229   32346 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8192 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/benjamin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0925 10:56:26.906374   32346 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0925 10:56:26.906452   32346 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0925 10:56:26.932031   32346 docker.go:685] Got preloaded images: -- stdout --
bitnami/kubectl:latest
jenkins/jenkins:2.462.2-jdk17
kiwigrid/k8s-sidecar:1.27.6
jenkins/inbound-agent:3261.v9c670a_4748a_9-2
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
jenkins/inbound-agent:3261.v9c670a_4748a_9-1
registry.k8s.io/etcd:3.5.15-0
docker:24-dind
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0925 10:56:26.932048   32346 docker.go:615] Images already preloaded, skipping extraction
I0925 10:56:26.932115   32346 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0925 10:56:26.957383   32346 docker.go:685] Got preloaded images: -- stdout --
bitnami/kubectl:latest
jenkins/jenkins:2.462.2-jdk17
kiwigrid/k8s-sidecar:1.27.6
jenkins/inbound-agent:3261.v9c670a_4748a_9-2
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
jenkins/inbound-agent:3261.v9c670a_4748a_9-1
registry.k8s.io/etcd:3.5.15-0
docker:24-dind
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0925 10:56:26.957399   32346 cache_images.go:84] Images are preloaded, skipping loading
I0925 10:56:26.957412   32346 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0925 10:56:26.957508   32346 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0925 10:56:26.957588   32346 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0925 10:56:27.010888   32346 cni.go:84] Creating CNI manager for ""
I0925 10:56:27.010915   32346 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0925 10:56:27.010950   32346 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0925 10:56:27.010981   32346 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0925 10:56:27.011147   32346 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0925 10:56:27.011245   32346 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0925 10:56:27.024774   32346 binaries.go:44] Found k8s binaries, skipping transfer
I0925 10:56:27.024863   32346 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0925 10:56:27.036877   32346 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0925 10:56:27.063713   32346 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0925 10:56:27.088644   32346 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2149 bytes)
I0925 10:56:27.117587   32346 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0925 10:56:27.122015   32346 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0925 10:56:27.139041   32346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0925 10:56:27.225637   32346 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0925 10:56:27.290473   32346 certs.go:68] Setting up /home/benjamin/.minikube/profiles/minikube for IP: 192.168.49.2
I0925 10:56:27.290493   32346 certs.go:194] generating shared ca certs ...
I0925 10:56:27.290517   32346 certs.go:226] acquiring lock for ca certs: {Name:mk1b6b6301b97055a5a21f5373be2ad2f5456731 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0925 10:56:27.290747   32346 certs.go:235] skipping valid "minikubeCA" ca cert: /home/benjamin/.minikube/ca.key
I0925 10:56:27.290800   32346 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/benjamin/.minikube/proxy-client-ca.key
I0925 10:56:27.290812   32346 certs.go:256] generating profile certs ...
I0925 10:56:27.290922   32346 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/benjamin/.minikube/profiles/minikube/client.key
I0925 10:56:27.290980   32346 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/benjamin/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0925 10:56:27.291027   32346 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/benjamin/.minikube/profiles/minikube/proxy-client.key
I0925 10:56:27.291182   32346 certs.go:484] found cert: /home/benjamin/.minikube/certs/ca-key.pem (1675 bytes)
I0925 10:56:27.291211   32346 certs.go:484] found cert: /home/benjamin/.minikube/certs/ca.pem (1082 bytes)
I0925 10:56:27.291238   32346 certs.go:484] found cert: /home/benjamin/.minikube/certs/cert.pem (1127 bytes)
I0925 10:56:27.291272   32346 certs.go:484] found cert: /home/benjamin/.minikube/certs/key.pem (1675 bytes)
I0925 10:56:27.292049   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0925 10:56:27.327712   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0925 10:56:27.362058   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0925 10:56:27.402243   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0925 10:56:27.435390   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0925 10:56:27.467730   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0925 10:56:27.501730   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0925 10:56:27.532260   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0925 10:56:27.564994   32346 ssh_runner.go:362] scp /home/benjamin/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0925 10:56:27.595315   32346 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0925 10:56:27.621768   32346 ssh_runner.go:195] Run: openssl version
I0925 10:56:27.631305   32346 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0925 10:56:27.645332   32346 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0925 10:56:27.652286   32346 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 17 18:33 /usr/share/ca-certificates/minikubeCA.pem
I0925 10:56:27.652371   32346 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0925 10:56:27.661101   32346 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0925 10:56:27.673524   32346 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0925 10:56:27.678665   32346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0925 10:56:27.686937   32346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0925 10:56:27.695205   32346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0925 10:56:27.704501   32346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0925 10:56:27.712316   32346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0925 10:56:27.719780   32346 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0925 10:56:27.727554   32346 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:8192 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/benjamin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0925 10:56:27.727748   32346 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0925 10:56:27.751829   32346 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0925 10:56:27.765925   32346 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0925 10:56:27.765942   32346 kubeadm.go:593] restartPrimaryControlPlane start ...
I0925 10:56:27.766035   32346 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0925 10:56:27.778073   32346 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0925 10:56:27.778483   32346 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/benjamin/.kube/config
I0925 10:56:27.778561   32346 kubeconfig.go:62] /home/benjamin/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0925 10:56:27.778751   32346 lock.go:35] WriteFile acquiring /home/benjamin/.kube/config: {Name:mkb9adc4bba6882cfc288f5637296b69776873e2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0925 10:56:27.779802   32346 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0925 10:56:27.791970   32346 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0925 10:56:27.792003   32346 kubeadm.go:597] duration metric: took 26.049032ms to restartPrimaryControlPlane
I0925 10:56:27.792017   32346 kubeadm.go:394] duration metric: took 64.478769ms to StartCluster
I0925 10:56:27.792040   32346 settings.go:142] acquiring lock: {Name:mke6f5d13fd6392d4789f7599d2f04c70531f22a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0925 10:56:27.792120   32346 settings.go:150] Updating kubeconfig:  /home/benjamin/.kube/config
I0925 10:56:27.792801   32346 lock.go:35] WriteFile acquiring /home/benjamin/.kube/config: {Name:mkb9adc4bba6882cfc288f5637296b69776873e2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0925 10:56:27.793120   32346 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0925 10:56:27.793254   32346 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0925 10:56:27.793230   32346 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0925 10:56:27.793285   32346 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0925 10:56:27.793284   32346 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0925 10:56:27.793318   32346 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0925 10:56:27.793318   32346 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0925 10:56:27.793324   32346 addons.go:243] addon storage-provisioner should already be in state true
I0925 10:56:27.793349   32346 host.go:66] Checking if "minikube" exists ...
I0925 10:56:27.793606   32346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0925 10:56:27.793712   32346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0925 10:56:27.795196   32346 out.go:177] 🔎  Verifying Kubernetes components...
I0925 10:56:27.796687   32346 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0925 10:56:27.815110   32346 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0925 10:56:27.815125   32346 addons.go:243] addon default-storageclass should already be in state true
I0925 10:56:27.815152   32346 host.go:66] Checking if "minikube" exists ...
I0925 10:56:27.815516   32346 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0925 10:56:27.815938   32346 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0925 10:56:27.817035   32346 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0925 10:56:27.817050   32346 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0925 10:56:27.817140   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:27.835049   32346 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0925 10:56:27.835066   32346 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0925 10:56:27.835158   32346 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0925 10:56:27.836323   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:27.856807   32346 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/benjamin/.minikube/machines/minikube/id_rsa Username:docker}
I0925 10:56:27.953334   32346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0925 10:56:27.966384   32346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0925 10:56:27.982833   32346 ssh_runner.go:195] Run: sudo systemctl start kubelet
W0925 10:56:28.027970   32346 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0925 10:56:28.028007   32346 retry.go:31] will retry after 365.74024ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0925 10:56:28.035056   32346 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0925 10:56:28.035087   32346 api_server.go:52] waiting for apiserver process to appear ...
I0925 10:56:28.035093   32346 retry.go:31] will retry after 215.682354ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0925 10:56:28.035167   32346 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0925 10:56:28.251489   32346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0925 10:56:28.327153   32346 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0925 10:56:28.327210   32346 retry.go:31] will retry after 345.828369ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0925 10:56:28.394344   32346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0925 10:56:28.535344   32346 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0925 10:56:28.673409   32346 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0925 10:56:31.117067   32346 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.722680822s)
I0925 10:56:31.117078   32346 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.581710977s)
I0925 10:56:31.117093   32346 api_server.go:72] duration metric: took 3.323949406s to wait for apiserver process to appear ...
I0925 10:56:31.117100   32346 api_server.go:88] waiting for apiserver healthz status ...
I0925 10:56:31.117124   32346 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0925 10:56:31.117152   32346 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.443709917s)
I0925 10:56:31.120081   32346 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0925 10:56:31.120101   32346 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0925 10:56:31.134529   32346 out.go:177] 🌟  Complementos habilitados: storage-provisioner, default-storageclass
I0925 10:56:31.136119   32346 addons.go:510] duration metric: took 3.342884221s for enable addons: enabled=[storage-provisioner default-storageclass]
I0925 10:56:31.617798   32346 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0925 10:56:31.621700   32346 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0925 10:56:31.621718   32346 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0925 10:56:32.117299   32346 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0925 10:56:32.121702   32346 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0925 10:56:32.122934   32346 api_server.go:141] control plane version: v1.31.0
I0925 10:56:32.122949   32346 api_server.go:131] duration metric: took 1.005841007s to wait for apiserver health ...
I0925 10:56:32.122955   32346 system_pods.go:43] waiting for kube-system pods to appear ...
I0925 10:56:32.128487   32346 system_pods.go:59] 7 kube-system pods found
I0925 10:56:32.128519   32346 system_pods.go:61] "coredns-6f6b679f8f-jv2k9" [f95c12c6-829a-493b-b396-24375c80b6aa] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0925 10:56:32.128526   32346 system_pods.go:61] "etcd-minikube" [c87ba86b-a23b-41d1-af44-19edc02641b5] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0925 10:56:32.128533   32346 system_pods.go:61] "kube-apiserver-minikube" [5b6d546c-8b45-4f8a-85aa-884017cbbb87] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0925 10:56:32.128540   32346 system_pods.go:61] "kube-controller-manager-minikube" [8dad4b8a-c0a1-4a19-9f0f-37deb8d780fe] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0925 10:56:32.128545   32346 system_pods.go:61] "kube-proxy-pm4zf" [4ed57ade-053e-4330-9758-fa1e66245407] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0925 10:56:32.128551   32346 system_pods.go:61] "kube-scheduler-minikube" [123caf6e-c276-4cd2-bcff-1168bf979380] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0925 10:56:32.128556   32346 system_pods.go:61] "storage-provisioner" [4909a6b1-3615-4342-b2d6-2fb5db1b03da] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0925 10:56:32.128563   32346 system_pods.go:74] duration metric: took 5.599899ms to wait for pod list to return data ...
I0925 10:56:32.128572   32346 kubeadm.go:582] duration metric: took 4.335429701s to wait for: map[apiserver:true system_pods:true]
I0925 10:56:32.128582   32346 node_conditions.go:102] verifying NodePressure condition ...
I0925 10:56:32.131973   32346 node_conditions.go:122] node storage ephemeral capacity is 489634808Ki
I0925 10:56:32.131990   32346 node_conditions.go:123] node cpu capacity is 12
I0925 10:56:32.132001   32346 node_conditions.go:105] duration metric: took 3.411565ms to run NodePressure ...
I0925 10:56:32.132011   32346 start.go:241] waiting for startup goroutines ...
I0925 10:56:32.132018   32346 start.go:246] waiting for cluster config update ...
I0925 10:56:32.132025   32346 start.go:255] writing updated cluster config ...
I0925 10:56:32.132248   32346 ssh_runner.go:195] Run: rm -f paused
I0925 10:56:32.176153   32346 start.go:600] kubectl: 1.31.0, cluster: 1.31.0 (minor skew: 0)
I0925 10:56:32.177761   32346 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 25 18:18:01 minikube cri-dockerd[1245]: time="2024-09-25T18:18:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/97e1fb46e55598b4426b7dd7541984bf1c65ed2cd5eb3222a8ac32bf457374bb/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:18:01 minikube dockerd[960]: time="2024-09-25T18:18:01.692991754Z" level=info msg="ignoring event" container=32baa8fddc4db8c34f545e9475100385be98fd543f1380d46200c8b4f775eb14 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:03 minikube dockerd[960]: time="2024-09-25T18:18:03.285999904Z" level=info msg="ignoring event" container=5fab1c4a1bd50502e014d07b977af6c19c15e2295b15ce37671e1e995bf242f0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:03 minikube dockerd[960]: time="2024-09-25T18:18:03.613790842Z" level=info msg="ignoring event" container=97e1fb46e55598b4426b7dd7541984bf1c65ed2cd5eb3222a8ac32bf457374bb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:11 minikube cri-dockerd[1245]: time="2024-09-25T18:18:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6792e73f008ee6b32b301cd419dd323cafe2fc4fb75684e6d31949b72991a898/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:18:11 minikube dockerd[960]: time="2024-09-25T18:18:11.593766907Z" level=info msg="ignoring event" container=0f4907b1a1af80fd8bfc351e4f4f052da791d3d30c8a450afbf61905fbc1cdd6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:13 minikube dockerd[960]: time="2024-09-25T18:18:13.485620648Z" level=info msg="ignoring event" container=199807fa0798b3c5a59929e735d45ce95122d8e846610a0d092fb8c094fd51bf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:13 minikube cri-dockerd[1245]: E0925 18:18:13.860373    1245 httpstream.go:144] (conn=&{0xc000c0a370 map[435:0xc0006f81e0 437:0xc0006f8be0] {0 0} 0x16340a0 0x1629060}, request=126) timed out waiting for streams
Sep 25 18:18:13 minikube dockerd[960]: time="2024-09-25T18:18:13.904850142Z" level=info msg="ignoring event" container=6792e73f008ee6b32b301cd419dd323cafe2fc4fb75684e6d31949b72991a898 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:18 minikube cri-dockerd[1245]: E0925 18:18:18.874147    1245 httpstream.go:144] (conn=&{0xc000c0a370 map[437:0xc0006f8be0 439:0xc00016f400] {0 0} 0x16340a0 0x1629060}, request=127) timed out waiting for streams
Sep 25 18:18:21 minikube cri-dockerd[1245]: time="2024-09-25T18:18:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f48aef6efb7328204074f1f9c115e4382769a2ce91b0deb3b8e08349a34d6b2/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:18:21 minikube dockerd[960]: time="2024-09-25T18:18:21.661780239Z" level=info msg="ignoring event" container=7f585ac7a115829969ee1cc02afb55aa0157c79718d1cb5cc8c71897a38d1c89 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:24 minikube dockerd[960]: time="2024-09-25T18:18:24.880709774Z" level=info msg="ignoring event" container=bd990cde519321eae72d7f9e3eb90855c6f3d042b846e62094b1c58029fb4510 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:25 minikube dockerd[960]: time="2024-09-25T18:18:25.253820487Z" level=info msg="ignoring event" container=7f48aef6efb7328204074f1f9c115e4382769a2ce91b0deb3b8e08349a34d6b2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:31 minikube cri-dockerd[1245]: time="2024-09-25T18:18:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e0aa1c76dd2c2704e81cf581a7fbb71a59ecc8fa017a4d947f2682591d1ee5f/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:18:31 minikube dockerd[960]: time="2024-09-25T18:18:31.638472863Z" level=info msg="ignoring event" container=8b4366c32f4a73a56e72aea569145f41064c04ef7dd2dd0ceae4c1c82e47f9dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:35 minikube dockerd[960]: time="2024-09-25T18:18:35.104079090Z" level=info msg="ignoring event" container=1756f8f1cb753088bf1699eb79be0361480d009d9166d0dc758f15e6ba84bea4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:35 minikube dockerd[960]: time="2024-09-25T18:18:35.426481112Z" level=info msg="ignoring event" container=0e0aa1c76dd2c2704e81cf581a7fbb71a59ecc8fa017a4d947f2682591d1ee5f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:41 minikube cri-dockerd[1245]: time="2024-09-25T18:18:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2f62726f29812217a7b3da0e1a6bbf1faff7551231802bfad417d560a22686f1/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:18:41 minikube dockerd[960]: time="2024-09-25T18:18:41.572039249Z" level=info msg="ignoring event" container=e2de08e488c501c868343cc783c0cd98f7f617817aebcd4dbe855128b62eb7ce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:43 minikube dockerd[960]: time="2024-09-25T18:18:43.240482087Z" level=info msg="ignoring event" container=2cd0536c09310ed690c45d54fa719d919fe989f1d52239cf25d90671797552fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:43 minikube dockerd[960]: time="2024-09-25T18:18:43.603319875Z" level=info msg="ignoring event" container=2f62726f29812217a7b3da0e1a6bbf1faff7551231802bfad417d560a22686f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:43 minikube cri-dockerd[1245]: E0925 18:18:43.863579    1245 httpstream.go:144] (conn=&{0xc000c0a370 map[439:0xc00016f400 441:0xc000ac0640] {0 0} 0x16340a0 0x1629060}, request=128) timed out waiting for streams
Sep 25 18:18:48 minikube cri-dockerd[1245]: E0925 18:18:48.879709    1245 httpstream.go:144] (conn=&{0xc000c0a370 map[441:0xc000ac0640] {0 0} 0x16340a0 0x1629060}, request=129) timed out waiting for streams
Sep 25 18:18:51 minikube cri-dockerd[1245]: time="2024-09-25T18:18:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6ed535e0fe7a83901eff55e719ec6e26eb7fdf38590d96a21889dba3c896d446/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:18:51 minikube dockerd[960]: time="2024-09-25T18:18:51.640024885Z" level=info msg="ignoring event" container=22c13bdf14c88b4ca06000283215342a8da09d539501050a2dc0650b1747ffcf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:53 minikube dockerd[960]: time="2024-09-25T18:18:53.424951543Z" level=info msg="ignoring event" container=3c1349154bc4092d82733eb40c5d060c8d9258905b304e6cd943dd8816f72c65 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:53 minikube dockerd[960]: time="2024-09-25T18:18:53.833237058Z" level=info msg="ignoring event" container=6ed535e0fe7a83901eff55e719ec6e26eb7fdf38590d96a21889dba3c896d446 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:18:58 minikube cri-dockerd[1245]: time="2024-09-25T18:18:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d162120a97bab2e26dcf1d352b41ee03bf333a39d28252c51e256f10645af70d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:01 minikube cri-dockerd[1245]: time="2024-09-25T18:19:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7520a16cb7d91c525bc1b01829224a4fafc4157220053ceb70764762915d448d/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:01 minikube dockerd[960]: time="2024-09-25T18:19:01.621219048Z" level=info msg="ignoring event" container=ed98f5957b451df5a7a2aab61fc1ce64927db11d45b0947f16e034abcad03c32 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:03 minikube dockerd[960]: time="2024-09-25T18:19:03.602210391Z" level=info msg="ignoring event" container=f5fe08d56ac133ce51bf5721366b3fe995f4438423e28b9c00b5810030305f86 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:03 minikube dockerd[960]: time="2024-09-25T18:19:03.975831062Z" level=info msg="ignoring event" container=7520a16cb7d91c525bc1b01829224a4fafc4157220053ceb70764762915d448d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:10 minikube cri-dockerd[1245]: time="2024-09-25T18:19:10Z" level=info msg="Pulling image nginx:latest: a2318d6c47ec: Downloading [=======================>                           ]  13.91MB/29.13MB"
Sep 25 18:19:11 minikube cri-dockerd[1245]: time="2024-09-25T18:19:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/50586c3f7164b8bfd4f87c92467c080ef5271a867c10c7cfd223f1dd12811bc9/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:11 minikube dockerd[960]: time="2024-09-25T18:19:11.623384416Z" level=info msg="ignoring event" container=29e8a8994f8255a81f887c6dc079d190be2870334ed65017fceb8ba00f545a53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:14 minikube dockerd[960]: time="2024-09-25T18:19:14.856126647Z" level=info msg="ignoring event" container=2611bf3193ba251de7d322fc25c27ed6a162ee45d4c281b82cadbad3d4b69c79 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:15 minikube dockerd[960]: time="2024-09-25T18:19:15.197370968Z" level=info msg="ignoring event" container=50586c3f7164b8bfd4f87c92467c080ef5271a867c10c7cfd223f1dd12811bc9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:20 minikube cri-dockerd[1245]: time="2024-09-25T18:19:20Z" level=info msg="Pulling image nginx:latest: 095d327c79ae: Downloading [==========================================>        ]  35.88MB/41.88MB"
Sep 25 18:19:21 minikube cri-dockerd[1245]: time="2024-09-25T18:19:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c8e5c63f2754205ff142c496e24247ef0aa1b912669ac9550237582f0119a7d1/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:21 minikube dockerd[960]: time="2024-09-25T18:19:21.665079407Z" level=info msg="ignoring event" container=9e95343f82f858a1d250bd86b5346cc8b8e75d7fcf6be7589ddc20de500abc15 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:23 minikube cri-dockerd[1245]: time="2024-09-25T18:19:23Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"
Sep 25 18:19:25 minikube dockerd[960]: time="2024-09-25T18:19:25.092039406Z" level=info msg="ignoring event" container=1537ff35456b96f54b7bc9cc75c16a0b2338e4b5f4d6bd131faabd66f0440fc8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:25 minikube dockerd[960]: time="2024-09-25T18:19:25.462626709Z" level=info msg="ignoring event" container=c8e5c63f2754205ff142c496e24247ef0aa1b912669ac9550237582f0119a7d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:31 minikube cri-dockerd[1245]: time="2024-09-25T18:19:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b84acfd7b962e4d48fc07864c576c1767e094a3e290e3209066b81dc9b512a1/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:31 minikube dockerd[960]: time="2024-09-25T18:19:31.642197278Z" level=info msg="ignoring event" container=bed9473342620294cbdafd9b1dcb23f2b1da3bb1408b8701544d2369171e1a76 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:33 minikube dockerd[960]: time="2024-09-25T18:19:33.264970712Z" level=info msg="ignoring event" container=3137ec029a6f21149e6596a428d3cccb445e12fe6194c72651e371259080a36b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:33 minikube dockerd[960]: time="2024-09-25T18:19:33.664070301Z" level=info msg="ignoring event" container=9b84acfd7b962e4d48fc07864c576c1767e094a3e290e3209066b81dc9b512a1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:41 minikube cri-dockerd[1245]: time="2024-09-25T18:19:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5071730141d64719c7d7e1be94dbcec5ed6ea31b34867fbf4fb729614d84c4c7/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:41 minikube dockerd[960]: time="2024-09-25T18:19:41.630527622Z" level=info msg="ignoring event" container=67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:43 minikube dockerd[960]: time="2024-09-25T18:19:43.486249154Z" level=info msg="ignoring event" container=d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:43 minikube dockerd[960]: time="2024-09-25T18:19:43.906979688Z" level=info msg="ignoring event" container=5071730141d64719c7d7e1be94dbcec5ed6ea31b34867fbf4fb729614d84c4c7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:51 minikube cri-dockerd[1245]: time="2024-09-25T18:19:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f15aca1fc2d4fbe861be5a4f2a5a70f797a25fcfb5414ca8531588394bb1e058/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:19:51 minikube dockerd[960]: time="2024-09-25T18:19:51.679586858Z" level=info msg="ignoring event" container=dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:54 minikube dockerd[960]: time="2024-09-25T18:19:54.871079609Z" level=info msg="ignoring event" container=aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:19:55 minikube dockerd[960]: time="2024-09-25T18:19:55.310359331Z" level=info msg="ignoring event" container=f15aca1fc2d4fbe861be5a4f2a5a70f797a25fcfb5414ca8531588394bb1e058 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:20:01 minikube cri-dockerd[1245]: time="2024-09-25T18:20:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/18269bc061fecfec98c36e3daf7a86d097f3676784fe39899d0f09c0801bb8e6/resolv.conf as [nameserver 10.96.0.10 search jenkins.svc.cluster.local svc.cluster.local cluster.local lan options ndots:5]"
Sep 25 18:20:01 minikube dockerd[960]: time="2024-09-25T18:20:01.644978658Z" level=info msg="ignoring event" container=1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:20:05 minikube dockerd[960]: time="2024-09-25T18:20:05.120329227Z" level=info msg="ignoring event" container=d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 25 18:20:05 minikube dockerd[960]: time="2024-09-25T18:20:05.451628935Z" level=info msg="ignoring event" container=18269bc061fecfec98c36e3daf7a86d097f3676784fe39899d0f09c0801bb8e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                     CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
5e7b1ad869c8e       nginx@sha256:04ba374043ccd2fc5c593885c0eacddebabd5ca375f9323666f28dfd5a9710e3             45 seconds ago      Running             nginx                     0                   d162120a97bab       nginx-676b6c5bbc-fq7kh
a9b2a4bab3a12       docker@sha256:8d5039800a368057d99fc0a75167d80f345ac8650850509adc7fe25c64cba9dd            3 minutes ago       Running             dind                      0                   8a8ab8e5c3a6b       dind-deployment-d8d64669d-lxzwr
1e7431d16c07f       jenkins/jenkins@sha256:95313257a8cddbef83c74e3d577ea139aeae30c3c014ddcaa83a72b60409bbe1   About an hour ago   Running             jenkins                   0                   277256857fc2b       jenkins-7b7649b989-v24ww
b050f12770380       6e38f40d628db                                                                             About an hour ago   Running             storage-provisioner       9                   5996b473d3560       storage-provisioner
373e59ceb51cf       7d34bb1185ec6                                                                             About an hour ago   Running             config-reload             4                   2e5afb2bb2259       jenkins-0
2dafc974c9bbe       jenkins/jenkins@sha256:95313257a8cddbef83c74e3d577ea139aeae30c3c014ddcaa83a72b60409bbe1   About an hour ago   Running             jenkins                   6                   2e5afb2bb2259       jenkins-0
033b86ca7109b       jenkins/jenkins@sha256:95313257a8cddbef83c74e3d577ea139aeae30c3c014ddcaa83a72b60409bbe1   About an hour ago   Exited              init                      0                   2e5afb2bb2259       jenkins-0
165c68878acf9       7d34bb1185ec6                                                                             About an hour ago   Exited              config-reload-init        4                   2e5afb2bb2259       jenkins-0
1700d467f3fa2       cbb01a7bd410d                                                                             About an hour ago   Running             coredns                   4                   1ef56e873baaa       coredns-6f6b679f8f-jv2k9
bf42f0003288a       6e38f40d628db                                                                             About an hour ago   Exited              storage-provisioner       8                   5996b473d3560       storage-provisioner
92ad838747910       ad83b2ca7b09e                                                                             About an hour ago   Running             kube-proxy                4                   b83b354d6b614       kube-proxy-pm4zf
89f5b9565a564       2e96e5913fc06                                                                             About an hour ago   Running             etcd                      4                   433022ae475f5       etcd-minikube
641050b2d68cf       045733566833c                                                                             About an hour ago   Running             kube-controller-manager   4                   b297f0bd5114c       kube-controller-manager-minikube
c72bedf5ccc18       604f5db92eaa8                                                                             About an hour ago   Running             kube-apiserver            4                   eea96a9818c5a       kube-apiserver-minikube
001c4ca6fe645       1766f54c897f0                                                                             About an hour ago   Running             kube-scheduler            4                   aaf5233bf2cb0       kube-scheduler-minikube
ffbd8aed3f420       7d34bb1185ec6                                                                             About an hour ago   Exited              config-reload             3                   5c6df035a8410       jenkins-0
37f04cca6638f       jenkins/jenkins@sha256:95313257a8cddbef83c74e3d577ea139aeae30c3c014ddcaa83a72b60409bbe1   About an hour ago   Exited              jenkins                   5                   5c6df035a8410       jenkins-0
a8a166c349005       cbb01a7bd410d                                                                             About an hour ago   Exited              coredns                   3                   462b116fe8e21       coredns-6f6b679f8f-jv2k9
185da108c4a2b       ad83b2ca7b09e                                                                             About an hour ago   Exited              kube-proxy                3                   ba98501c81faf       kube-proxy-pm4zf
a6604c9e30e4a       604f5db92eaa8                                                                             About an hour ago   Exited              kube-apiserver            3                   c401e2945e3da       kube-apiserver-minikube
271aa98071192       2e96e5913fc06                                                                             About an hour ago   Exited              etcd                      3                   02dcca22c4f0a       etcd-minikube
7bdcf16a3c9d3       1766f54c897f0                                                                             About an hour ago   Exited              kube-scheduler            3                   058f2ee36a658       kube-scheduler-minikube
d4e9a6576b489       045733566833c                                                                             About an hour ago   Exited              kube-controller-manager   3                   dbc76a7eff248       kube-controller-manager-minikube


==> coredns [1700d467f3fa] <==
[INFO] 10.244.0.200:41590 - 60970 "AAAA IN jenkins.jenkins.svc.cluster.local.lan. udp 55 false 512" NXDOMAIN qr,aa,rd,ra 55 0.043917644s
[INFO] 10.244.0.200:58924 - 39515 "A IN jenkins.jenkins.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000140312s
[INFO] 10.244.0.200:58924 - 30242 "AAAA IN jenkins.jenkins.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000187873s
[INFO] 10.244.0.200:54055 - 50201 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000221887s
[INFO] 10.244.0.200:54055 - 44816 "A IN jenkins-agent.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000340617s
[INFO] 10.244.0.200:33590 - 4541 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000121664s
[INFO] 10.244.0.200:33590 - 17845 "A IN jenkins-agent.jenkins.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000153511s
[INFO] 10.244.0.200:55809 - 36736 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.00004819s
[INFO] 10.244.0.200:55809 - 52875 "A IN jenkins-agent.jenkins.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000058736s
[INFO] 10.244.0.200:37125 - 37101 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.lan. udp 61 false 512" NXDOMAIN qr,aa,rd,ra 61 0.012315565s
[INFO] 10.244.0.200:37125 - 11509 "A IN jenkins-agent.jenkins.svc.cluster.local.lan. udp 61 false 512" NXDOMAIN qr,aa,rd,ra 61 0.012363546s
[INFO] 10.244.0.200:33867 - 60408 "A IN jenkins-agent.jenkins.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000080108s
[INFO] 10.244.0.200:33867 - 52973 "AAAA IN jenkins-agent.jenkins.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000139334s
[INFO] 10.244.0.96:58333 - 50459 "PTR IN 200.0.244.10.in-addr.arpa. udp 43 false 512" NXDOMAIN qr,aa,rd,ra 43 0.003774094s
[INFO] 10.244.0.201:37708 - 15152 "AAAA IN jenkins.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000331189s
[INFO] 10.244.0.201:37708 - 65038 "A IN jenkins.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000447964s
[INFO] 10.244.0.201:35502 - 63827 "A IN jenkins.jenkins.svc.cluster.local.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000095403s
[INFO] 10.244.0.201:35502 - 37466 "AAAA IN jenkins.jenkins.svc.cluster.local.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000124038s
[INFO] 10.244.0.201:59699 - 20832 "A IN jenkins.jenkins.svc.cluster.local.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.00007927s
[INFO] 10.244.0.201:59699 - 51801 "AAAA IN jenkins.jenkins.svc.cluster.local.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000138216s
[INFO] 10.244.0.201:51694 - 30005 "A IN jenkins.jenkins.svc.cluster.local.lan. udp 55 false 512" NXDOMAIN qr,aa,rd,ra 55 0.009918816s
[INFO] 10.244.0.201:51694 - 64302 "AAAA IN jenkins.jenkins.svc.cluster.local.lan. udp 55 false 512" NXDOMAIN qr,aa,rd,ra 55 0.010045578s
[INFO] 10.244.0.201:35898 - 11841 "A IN jenkins.jenkins.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000114609s
[INFO] 10.244.0.201:35898 - 8828 "AAAA IN jenkins.jenkins.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000178864s
[INFO] 10.244.0.201:49503 - 23093 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.00019821s
[INFO] 10.244.0.201:49503 - 24264 "A IN jenkins-agent.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000288585s
[INFO] 10.244.0.201:58047 - 4225 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.00011049s
[INFO] 10.244.0.201:58047 - 42638 "A IN jenkins-agent.jenkins.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000168318s
[INFO] 10.244.0.201:53359 - 32558 "A IN jenkins-agent.jenkins.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000065301s
[INFO] 10.244.0.201:53359 - 15157 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000097289s
[INFO] 10.244.0.201:40397 - 16746 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.lan. udp 61 false 512" NXDOMAIN qr,aa,rd,ra 61 0.010125755s
[INFO] 10.244.0.201:40397 - 65394 "A IN jenkins-agent.jenkins.svc.cluster.local.lan. udp 61 false 512" NXDOMAIN qr,aa,rd,ra 61 0.010313699s
[INFO] 10.244.0.201:43025 - 45265 "AAAA IN jenkins-agent.jenkins.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000231035s
[INFO] 10.244.0.201:43025 - 17383 "A IN jenkins-agent.jenkins.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000243468s
[INFO] 10.244.0.96:39973 - 11265 "PTR IN 201.0.244.10.in-addr.arpa. udp 43 false 512" NXDOMAIN qr,aa,rd,ra 43 0.00529992s
[INFO] 10.244.0.96:54949 - 58617 "AAAA IN kubernetes.default.jenkins.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000352071s
[INFO] 10.244.0.96:54949 - 4594 "A IN kubernetes.default.jenkins.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000467798s
[INFO] 10.244.0.96:42996 - 29780 "AAAA IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 147 0.000244445s
[INFO] 10.244.0.96:42996 - 41828 "A IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000329792s
[INFO] 10.244.0.202:56144 - 56922 "AAAA IN jenkins.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000274198s
[INFO] 10.244.0.202:56144 - 2914 "A IN jenkins.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000316801s
[INFO] 10.244.0.202:43219 - 58433 "AAAA IN jenkins.jenkins.svc.cluster.local.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000053009s
[INFO] 10.244.0.202:43219 - 40268 "A IN jenkins.jenkins.svc.cluster.local.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000091563s
[INFO] 10.244.0.202:39276 - 32024 "AAAA IN jenkins.jenkins.svc.cluster.local.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000052172s
[INFO] 10.244.0.202:39276 - 12555 "A IN jenkins.jenkins.svc.cluster.local.cluster.local. udp 65 false 512" NXDOMAIN qr,aa,rd 158 0.000096242s
[INFO] 10.244.0.202:52757 - 55965 "A IN jenkins.jenkins.svc.cluster.local.lan. udp 55 false 512" NXDOMAIN qr,aa,rd,ra 55 0.009033505s
[INFO] 10.244.0.202:52757 - 1765 "AAAA IN jenkins.jenkins.svc.cluster.local.lan. udp 55 false 512" NXDOMAIN qr,aa,rd,ra 55 0.009127931s
[INFO] 10.244.0.202:59969 - 35447 "A IN jenkins.jenkins.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000114191s
[INFO] 10.244.0.202:59969 - 46946 "AAAA IN jenkins.jenkins.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000188432s
[INFO] 10.244.0.202:58845 - 36175 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.00013675s
[INFO] 10.244.0.202:58845 - 63297 "A IN jenkins-agent.jenkins.svc.cluster.local.jenkins.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000234947s
[INFO] 10.244.0.202:34635 - 64023 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000113492s
[INFO] 10.244.0.202:34635 - 45329 "A IN jenkins-agent.jenkins.svc.cluster.local.svc.cluster.local. udp 75 false 512" NXDOMAIN qr,aa,rd 168 0.000182775s
[INFO] 10.244.0.202:59100 - 4641 "A IN jenkins-agent.jenkins.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000044908s
[INFO] 10.244.0.202:59100 - 17445 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000091772s
[INFO] 10.244.0.202:40132 - 15849 "A IN jenkins-agent.jenkins.svc.cluster.local.lan. udp 61 false 512" NXDOMAIN qr,aa,rd,ra 61 0.004221218s
[INFO] 10.244.0.202:40132 - 35313 "AAAA IN jenkins-agent.jenkins.svc.cluster.local.lan. udp 61 false 512" NXDOMAIN qr,aa,rd,ra 61 0.004286032s
[INFO] 10.244.0.202:33839 - 20494 "AAAA IN jenkins-agent.jenkins.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000054686s
[INFO] 10.244.0.202:33839 - 49408 "A IN jenkins-agent.jenkins.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000086673s
[INFO] 10.244.0.96:33432 - 62526 "PTR IN 202.0.244.10.in-addr.arpa. udp 43 false 512" NXDOMAIN qr,aa,rd,ra 43 0.003792113s


==> coredns [a8a166c34900] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:56594 - 41079 "HINFO IN 1484592185401177047.1279040197906950598. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.045537749s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1129541836]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (25-Sep-2024 16:51:52.062) (total time: 30001ms):
Trace[1129541836]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (16:52:22.062)
Trace[1129541836]: [30.001041536s] [30.001041536s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[2033691786]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (25-Sep-2024 16:51:52.062) (total time: 30001ms):
Trace[2033691786]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (16:52:22.062)
Trace[2033691786]: [30.001011016s] [30.001011016s] END
[INFO] plugin/kubernetes: Trace[1143353971]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (25-Sep-2024 16:51:52.062) (total time: 30001ms):
Trace[1143353971]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (16:52:22.062)
Trace[1143353971]: [30.001287577s] [30.001287577s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_24T11_19_22_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 24 Sep 2024 17:19:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 25 Sep 2024 18:20:03 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 25 Sep 2024 18:19:38 +0000   Tue, 24 Sep 2024 17:19:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 25 Sep 2024 18:19:38 +0000   Tue, 24 Sep 2024 17:19:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 25 Sep 2024 18:19:38 +0000   Tue, 24 Sep 2024 17:19:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 25 Sep 2024 18:19:38 +0000   Tue, 24 Sep 2024 17:19:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  489634808Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             14151488Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  489634808Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             14151488Ki
  pods:               110
System Info:
  Machine ID:                 ff93de7cba8244f79e05a067b040e628
  System UUID:                b48a0c7f-afc1-49d3-8ca2-917c8b746150
  Boot ID:                    aaa97e8b-ffa1-44e5-b93d-2f624cd7f7c1
  Kernel Version:             6.1.0-25-amd64
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     dind-deployment-d8d64669d-lxzwr     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m39s
  default                     jenkins-7b7649b989-v24ww            0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m
  default                     jenkins-7d469b7bd8-bvjwx            0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m
  default                     nginx-676b6c5bbc-fq7kh              0 (0%)        0 (0%)      0 (0%)           0 (0%)         72s
  jenkins                     jenkins-0                           50m (0%)      2 (16%)     256Mi (1%)       4Gi (29%)      24h
  kube-system                 coredns-6f6b679f8f-jv2k9            100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     25h
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         25h
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         25h
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         25h
  kube-system                 kube-proxy-pm4zf                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         25h
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         25h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         25h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                800m (6%)   2 (16%)
  memory             426Mi (3%)  4266Mi (30%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Sep25 16:23] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.138074] Unstable clock detected, switching default tracing clock to "global"
              If you want to keep using the local clock, then add:
                "trace_clock=local"
              on the kernel command line
[  +2.049494] amdgpu 0000:04:00.0: amdgpu: PSP runtime database doesn't exist
[  +0.000002] amdgpu 0000:04:00.0: amdgpu: PSP runtime database doesn't exist
[  +0.807587] [drm] psp gfx command LOAD_TA(0x1) failed and response status is (0x7)
[  +0.000219] [drm] psp gfx command INVOKE_CMD(0x3) failed and response status is (0x4)
[  +0.000002] amdgpu 0000:04:00.0: amdgpu: Secure display: Generic Failure.
[  +0.000007] amdgpu 0000:04:00.0: amdgpu: SECUREDISPLAY: query securedisplay TA failed. ret 0x0
[  +0.204702] amdgpu: SRAT table not found
[  +0.749919] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.036038] vboxdrv: loading out-of-tree module taints kernel.
[  +0.247226] VBoxNetFlt: Successfully started.
[  +0.003952] VBoxNetAdp: Successfully started.
[  +0.944379] Bluetooth: hci0: HCI Enhanced Setup Synchronous Connection command is advertised, but not supported.
[  +5.539944] kauditd_printk_skb: 31 callbacks suppressed
[ +37.132192] tmpfs: Unknown parameter 'noswap'
[Sep25 16:51] tmpfs: Unknown parameter 'noswap'
[Sep25 16:56] tmpfs: Unknown parameter 'noswap'


==> etcd [271aa9807119] <==
{"level":"info","ts":"2024-09-25T16:51:49.160766Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-09-25T16:51:49.160837Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-09-25T16:51:49.160857Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-09-25T16:51:49.160868Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-25T16:51:49.160889Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-25T16:51:49.161423Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-09-25T16:51:49.161564Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-09-25T16:51:49.164031Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.240079ms"}
{"level":"info","ts":"2024-09-25T16:51:49.222944Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2024-09-25T16:51:49.223032Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":4304896,"backend-size":"4.3 MB","backend-size-in-use-bytes":1863680,"backend-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-09-25T16:51:49.309190Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":25020}
{"level":"info","ts":"2024-09-25T16:51:49.309749Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-09-25T16:51:49.309787Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2024-09-25T16:51:49.309800Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 4, commit: 25020, applied: 20002, lastindex: 25020, lastterm: 4]"}
{"level":"info","ts":"2024-09-25T16:51:49.309923Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-25T16:51:49.309946Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-25T16:51:49.309962Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-09-25T16:51:49.310843Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-09-25T16:51:49.311402Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":19927}
{"level":"info","ts":"2024-09-25T16:51:49.317897Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":20287}
{"level":"info","ts":"2024-09-25T16:51:49.320420Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-09-25T16:51:49.322663Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-09-25T16:51:49.323525Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-09-25T16:51:49.323579Z","caller":"etcdserver/server.go:858","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-25T16:51:49.323673Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-09-25T16:51:49.323847Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-25T16:51:49.324051Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-25T16:51:49.324124Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-25T16:51:49.324270Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-25T16:51:49.325362Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-25T16:51:49.325499Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-25T16:51:49.325563Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-25T16:51:49.325620Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-09-25T16:51:49.325655Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-09-25T16:51:50.011253Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2024-09-25T16:51:50.011346Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2024-09-25T16:51:50.011482Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-09-25T16:51:50.011510Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2024-09-25T16:51:50.011532Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-09-25T16:51:50.011555Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2024-09-25T16:51:50.011578Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-09-25T16:51:50.016320Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-09-25T16:51:50.016349Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-25T16:51:50.016411Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-25T16:51:50.016636Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-09-25T16:51:50.016702Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-09-25T16:51:50.017693Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-25T16:51:50.017997Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-25T16:51:50.018407Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-09-25T16:51:50.019577Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-09-25T16:55:10.441157Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-09-25T16:55:10.441288Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-09-25T16:55:10.441445Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-25T16:55:10.441635Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-25T16:55:10.456688Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-25T16:55:10.456764Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-09-25T16:55:10.456872Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-09-25T16:55:10.459770Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-25T16:55:10.460032Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-25T16:55:10.460049Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [89f5b9565a56] <==
{"level":"info","ts":"2024-09-25T16:56:29.396832Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-09-25T16:56:29.396857Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-09-25T16:56:29.396885Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-09-25T16:56:29.400433Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-25T16:56:29.400559Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-25T16:56:29.400432Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-09-25T16:56:29.400921Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-09-25T16:56:29.401062Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-09-25T16:56:29.402091Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-25T16:56:29.402097Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-25T16:56:29.403391Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-09-25T16:56:29.403686Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-09-25T17:06:29.431569Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20882}
{"level":"info","ts":"2024-09-25T17:06:29.451622Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":20882,"took":"19.545237ms","hash":3985881321,"current-db-size-bytes":4423680,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":3489792,"current-db-size-in-use":"3.5 MB"}
{"level":"info","ts":"2024-09-25T17:06:29.451702Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3985881321,"revision":20882,"compact-revision":19927}
{"level":"info","ts":"2024-09-25T17:11:29.440403Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21149}
{"level":"info","ts":"2024-09-25T17:11:29.459019Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":21149,"took":"18.055177ms","hash":663549891,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-09-25T17:11:29.459106Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":663549891,"revision":21149,"compact-revision":20882}
{"level":"info","ts":"2024-09-25T17:16:29.449522Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21390}
{"level":"info","ts":"2024-09-25T17:16:29.465639Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":21390,"took":"15.673657ms","hash":57707684,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-09-25T17:16:29.465732Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":57707684,"revision":21390,"compact-revision":21149}
{"level":"info","ts":"2024-09-25T17:21:29.454857Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21629}
{"level":"info","ts":"2024-09-25T17:21:29.470468Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":21629,"took":"15.186598ms","hash":1677789085,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2048000,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:21:29.470527Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1677789085,"revision":21629,"compact-revision":21390}
{"level":"info","ts":"2024-09-25T17:26:29.462528Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21922}
{"level":"info","ts":"2024-09-25T17:26:29.479601Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":21922,"took":"16.394699ms","hash":4048600925,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2105344,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-09-25T17:26:29.479679Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4048600925,"revision":21922,"compact-revision":21629}
{"level":"info","ts":"2024-09-25T17:31:29.471118Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22179}
{"level":"info","ts":"2024-09-25T17:31:29.487403Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":22179,"took":"15.77923ms","hash":1382786416,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":1994752,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:31:29.487489Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1382786416,"revision":22179,"compact-revision":21922}
{"level":"info","ts":"2024-09-25T17:36:29.479651Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22421}
{"level":"info","ts":"2024-09-25T17:36:29.494609Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":22421,"took":"14.585592ms","hash":3363115653,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":1982464,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:36:29.494665Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3363115653,"revision":22421,"compact-revision":22179}
{"level":"info","ts":"2024-09-25T17:41:29.486624Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22662}
{"level":"info","ts":"2024-09-25T17:41:29.502109Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":22662,"took":"15.115313ms","hash":2135863766,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2019328,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:41:29.502195Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2135863766,"revision":22662,"compact-revision":22421}
{"level":"info","ts":"2024-09-25T17:46:29.494743Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22920}
{"level":"info","ts":"2024-09-25T17:46:29.511642Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":22920,"took":"16.304095ms","hash":750445503,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2023424,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:46:29.511725Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":750445503,"revision":22920,"compact-revision":22662}
{"level":"info","ts":"2024-09-25T17:51:29.503515Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23161}
{"level":"info","ts":"2024-09-25T17:51:29.520064Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":23161,"took":"16.037887ms","hash":2138373125,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2002944,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:51:29.520146Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2138373125,"revision":23161,"compact-revision":22920}
{"level":"info","ts":"2024-09-25T17:56:29.511555Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23405}
{"level":"info","ts":"2024-09-25T17:56:29.527118Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":23405,"took":"15.009168ms","hash":4009019027,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":1994752,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T17:56:29.527173Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4009019027,"revision":23405,"compact-revision":23161}
{"level":"info","ts":"2024-09-25T18:01:29.519570Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23646}
{"level":"info","ts":"2024-09-25T18:01:29.536023Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":23646,"took":"15.849147ms","hash":2995953598,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2035712,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-09-25T18:01:29.536105Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2995953598,"revision":23646,"compact-revision":23405}
{"level":"info","ts":"2024-09-25T18:05:32.652024Z","caller":"etcdserver/server.go:1451","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":30003,"local-member-snapshot-index":20002,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-09-25T18:05:32.657973Z","caller":"etcdserver/server.go:2471","msg":"saved snapshot","snapshot-index":30003}
{"level":"info","ts":"2024-09-25T18:05:32.658314Z","caller":"etcdserver/server.go:2501","msg":"compacted Raft logs","compact-index":25003}
{"level":"info","ts":"2024-09-25T18:06:29.525861Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23908}
{"level":"info","ts":"2024-09-25T18:06:29.542641Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":23908,"took":"16.304957ms","hash":3217642472,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":2654208,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2024-09-25T18:06:29.542706Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3217642472,"revision":23908,"compact-revision":23646}
{"level":"info","ts":"2024-09-25T18:11:29.534678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24456}
{"level":"info","ts":"2024-09-25T18:11:29.538893Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":24456,"took":"3.594047ms","hash":862557394,"current-db-size-bytes":4567040,"current-db-size":"4.6 MB","current-db-size-in-use-bytes":4427776,"current-db-size-in-use":"4.4 MB"}
{"level":"info","ts":"2024-09-25T18:11:29.538945Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":862557394,"revision":24456,"compact-revision":23908}
{"level":"info","ts":"2024-09-25T18:16:29.542156Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25181}
{"level":"info","ts":"2024-09-25T18:16:29.562409Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":25181,"took":"19.525657ms","hash":1776875067,"current-db-size-bytes":5406720,"current-db-size":"5.4 MB","current-db-size-in-use-bytes":3923968,"current-db-size-in-use":"3.9 MB"}
{"level":"info","ts":"2024-09-25T18:16:29.562489Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1776875067,"revision":25181,"compact-revision":24456}


==> kernel <==
 18:20:09 up  1:57,  0 users,  load average: 1.32, 1.25, 1.18
Linux minikube 6.1.0-25-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.106-3 (2024-08-26) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [a6604c9e30e4] <==
I0925 16:55:10.453067       1 controller.go:176] quota evaluator worker shutdown
I0925 16:55:10.453083       1 controller.go:176] quota evaluator worker shutdown
I0925 16:55:10.453101       1 controller.go:176] quota evaluator worker shutdown
W0925 16:55:10.454360       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447336       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447389       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447338       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447450       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447467       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447480       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447499       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447471       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447546       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447553       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447713       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447762       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447797       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447802       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447814       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447886       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447974       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.447990       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448012       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448018       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448068       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448089       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448093       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448129       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448133       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448152       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448184       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448251       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448329       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448598       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448668       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448696       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.448734       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.449964       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450046       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450085       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450100       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450100       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450138       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450169       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450171       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450177       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450329       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450432       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450525       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450590       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450631       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450631       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450728       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.450942       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.452332       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.452349       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.452332       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.452446       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.452449       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0925 16:55:11.455004       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [c72bedf5ccc1] <==
I0925 16:56:30.422372       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0925 16:56:30.422477       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0925 16:56:30.422512       1 secure_serving.go:213] Serving securely on [::]:8443
I0925 16:56:30.422520       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0925 16:56:30.422593       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0925 16:56:30.422733       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0925 16:56:30.422784       1 local_available_controller.go:156] Starting LocalAvailability controller
I0925 16:56:30.423094       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0925 16:56:30.423094       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0925 16:56:30.422930       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0925 16:56:30.423804       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0925 16:56:30.423964       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0925 16:56:30.423000       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0925 16:56:30.424317       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0925 16:56:30.424449       1 crd_finalizer.go:269] Starting CRDFinalizer
I0925 16:56:30.424541       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0925 16:56:30.423869       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0925 16:56:30.424811       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0925 16:56:30.423020       1 controller.go:78] Starting OpenAPI AggregationController
I0925 16:56:30.424321       1 controller.go:142] Starting OpenAPI controller
I0925 16:56:30.424676       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0925 16:56:30.423096       1 controller.go:119] Starting legacy_token_tracking_controller
I0925 16:56:30.426545       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0925 16:56:30.423195       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0925 16:56:30.422857       1 aggregator.go:169] waiting for initial CRD sync...
I0925 16:56:30.423271       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0925 16:56:30.423066       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0925 16:56:30.423221       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0925 16:56:30.438220       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0925 16:56:30.424358       1 controller.go:90] Starting OpenAPI V3 controller
I0925 16:56:30.424376       1 naming_controller.go:294] Starting NamingConditionController
I0925 16:56:30.424397       1 establishing_controller.go:81] Starting EstablishingController
I0925 16:56:30.424415       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0925 16:56:30.424431       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0925 16:56:30.475622       1 shared_informer.go:320] Caches are synced for node_authorizer
I0925 16:56:30.479749       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0925 16:56:30.479788       1 policy_source.go:224] refreshing policies
I0925 16:56:30.522952       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0925 16:56:30.523007       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0925 16:56:30.524059       1 cache.go:39] Caches are synced for LocalAvailability controller
I0925 16:56:30.524249       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0925 16:56:30.524312       1 aggregator.go:171] initial CRD sync complete...
I0925 16:56:30.524333       1 autoregister_controller.go:144] Starting autoregister controller
I0925 16:56:30.524347       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0925 16:56:30.524362       1 cache.go:39] Caches are synced for autoregister controller
I0925 16:56:30.524760       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0925 16:56:30.525158       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0925 16:56:30.527009       1 shared_informer.go:320] Caches are synced for configmaps
I0925 16:56:30.528912       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0925 16:56:30.539230       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0925 16:56:30.542936       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0925 16:56:31.428255       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0925 16:56:34.150782       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0925 16:56:34.200616       1 controller.go:615] quota admission added evaluator for: endpoints
I0925 17:05:39.927392       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0925 17:05:39.936122       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0925 17:05:39.952771       1 alloc.go:330] "allocated clusterIPs" service="default/jenkins" clusterIPs={"IPv4":"10.110.20.154"}
E0925 17:19:38.856053       1 upgradeaware.go:441] Error proxying data from backend to client: websocket: close sent
E0925 18:18:26.931236       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:33536: use of closed network connection
I0925 18:19:09.032288       1 alloc.go:330] "allocated clusterIPs" service="default/nginx" clusterIPs={"IPv4":"10.98.150.139"}


==> kube-controller-manager [641050b2d68c] <==
I0925 16:56:33.962290       1 shared_informer.go:320] Caches are synced for stateful set
I0925 16:56:33.996403       1 shared_informer.go:320] Caches are synced for disruption
I0925 16:56:34.007344       1 shared_informer.go:320] Caches are synced for resource quota
I0925 16:56:34.015116       1 shared_informer.go:320] Caches are synced for resource quota
I0925 16:56:34.422737       1 shared_informer.go:320] Caches are synced for garbage collector
I0925 16:56:34.459774       1 shared_informer.go:320] Caches are synced for garbage collector
I0925 16:56:34.459810       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0925 16:57:09.984591       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="10.962768ms"
I0925 16:57:09.984749       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="79.687µs"
I0925 17:01:37.184530       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:05:39.956525       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7b7649b989" duration="17.36779ms"
I0925 17:05:39.963612       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7b7649b989" duration="6.986282ms"
I0925 17:05:39.963778       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7b7649b989" duration="82.41µs"
I0925 17:05:39.972144       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7b7649b989" duration="517.228µs"
I0925 17:05:42.606251       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7b7649b989" duration="6.871676ms"
I0925 17:05:42.606381       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7b7649b989" duration="76.404µs"
I0925 17:06:02.902214       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:11:08.408502       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:16:13.910578       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:20:09.527582       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="15.183284ms"
I0925 17:20:09.542106       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="14.439014ms"
I0925 17:20:09.542291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="127.817µs"
I0925 17:20:28.786749       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7d469b7bd8" duration="5.890525ms"
I0925 17:20:28.786895       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7d469b7bd8" duration="78.925µs"
I0925 17:20:28.804695       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="12.33576ms"
I0925 17:20:28.813050       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="8.278598ms"
I0925 17:20:28.813149       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="49.66µs"
I0925 17:20:28.820104       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7d469b7bd8" duration="15.529565ms"
I0925 17:20:28.826793       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7d469b7bd8" duration="6.61859ms"
I0925 17:20:28.826946       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7d469b7bd8" duration="90.589µs"
I0925 17:20:28.831906       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-7d469b7bd8" duration="58.251µs"
I0925 17:20:30.129000       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="127.607µs"
I0925 17:20:30.137121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="62.232µs"
I0925 17:20:30.139730       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/jenkins-857dff6cf6" duration="152.751µs"
I0925 17:21:19.369885       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:26:25.695735       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:31:31.942806       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:36:38.500907       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:41:45.077897       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:46:51.094409       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:51:57.702333       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 17:57:03.587864       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 18:02:09.931329       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 18:03:30.915054       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 18:08:36.359775       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 18:13:41.127634       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 18:16:30.296180       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dind-deployment-d8d64669d" duration="25.856858ms"
I0925 18:16:30.303885       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dind-deployment-d8d64669d" duration="7.630044ms"
I0925 18:16:30.304020       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dind-deployment-d8d64669d" duration="69.212µs"
I0925 18:16:30.310673       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dind-deployment-d8d64669d" duration="138.705µs"
I0925 18:16:32.916620       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dind-deployment-d8d64669d" duration="5.051356ms"
I0925 18:16:32.916703       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/dind-deployment-d8d64669d" duration="39.67µs"
I0925 18:17:05.728579       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 18:18:57.811382       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-676b6c5bbc" duration="18.76843ms"
I0925 18:18:57.819397       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-676b6c5bbc" duration="7.871274ms"
I0925 18:18:57.819545       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-676b6c5bbc" duration="58.947µs"
I0925 18:18:57.825955       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-676b6c5bbc" duration="65.023µs"
I0925 18:19:23.725167       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-676b6c5bbc" duration="4.963214ms"
I0925 18:19:23.725225       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-676b6c5bbc" duration="26.749µs"
I0925 18:19:38.139307       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [d4e9a6576b48] <==
I0925 16:51:54.128689       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I0925 16:51:54.128724       1 shared_informer.go:313] Waiting for caches to sync for crt configmap
I0925 16:51:54.229317       1 controllermanager.go:797] "Started controller" controller="validatingadmissionpolicy-status-controller"
I0925 16:51:54.229455       1 shared_informer.go:313] Waiting for caches to sync for validatingadmissionpolicy-status
I0925 16:51:54.235977       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0925 16:51:54.245144       1 shared_informer.go:320] Caches are synced for cronjob
I0925 16:51:54.245827       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0925 16:51:54.247278       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0925 16:51:54.250511       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0925 16:51:54.254812       1 shared_informer.go:320] Caches are synced for expand
I0925 16:51:54.258081       1 shared_informer.go:320] Caches are synced for TTL after finished
I0925 16:51:54.260308       1 shared_informer.go:320] Caches are synced for endpoint
I0925 16:51:54.278437       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0925 16:51:54.278487       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0925 16:51:54.279744       1 shared_informer.go:320] Caches are synced for service account
I0925 16:51:54.279781       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0925 16:51:54.279859       1 shared_informer.go:320] Caches are synced for PV protection
I0925 16:51:54.279862       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0925 16:51:54.280960       1 shared_informer.go:320] Caches are synced for daemon sets
I0925 16:51:54.283250       1 shared_informer.go:320] Caches are synced for persistent volume
I0925 16:51:54.285697       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0925 16:51:54.286976       1 shared_informer.go:320] Caches are synced for HPA
I0925 16:51:54.287794       1 shared_informer.go:320] Caches are synced for node
I0925 16:51:54.287885       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I0925 16:51:54.287928       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I0925 16:51:54.287950       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0925 16:51:54.287964       1 shared_informer.go:320] Caches are synced for cidrallocator
I0925 16:51:54.288075       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0925 16:51:54.307739       1 shared_informer.go:320] Caches are synced for namespace
I0925 16:51:54.311464       1 shared_informer.go:320] Caches are synced for job
I0925 16:51:54.317882       1 shared_informer.go:320] Caches are synced for taint
I0925 16:51:54.318004       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0925 16:51:54.318177       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0925 16:51:54.318274       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0925 16:51:54.324384       1 shared_informer.go:320] Caches are synced for ephemeral
I0925 16:51:54.327156       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0925 16:51:54.329539       1 shared_informer.go:320] Caches are synced for GC
I0925 16:51:54.329594       1 shared_informer.go:320] Caches are synced for PVC protection
I0925 16:51:54.329593       1 shared_informer.go:320] Caches are synced for crt configmap
I0925 16:51:54.329647       1 shared_informer.go:320] Caches are synced for TTL
I0925 16:51:54.329745       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0925 16:51:54.329781       1 shared_informer.go:320] Caches are synced for deployment
I0925 16:51:54.329995       1 shared_informer.go:320] Caches are synced for stateful set
I0925 16:51:54.330065       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0925 16:51:54.334605       1 shared_informer.go:320] Caches are synced for attach detach
I0925 16:51:54.337460       1 shared_informer.go:320] Caches are synced for ReplicationController
I0925 16:51:54.343166       1 shared_informer.go:320] Caches are synced for disruption
I0925 16:51:54.379747       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0925 16:51:54.379957       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0925 16:51:54.430138       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0925 16:51:54.430207       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0925 16:51:54.536096       1 shared_informer.go:320] Caches are synced for resource quota
I0925 16:51:54.578889       1 shared_informer.go:320] Caches are synced for resource quota
I0925 16:51:54.738195       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="408.373102ms"
I0925 16:51:54.738455       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="131.227µs"
I0925 16:51:54.951172       1 shared_informer.go:320] Caches are synced for garbage collector
I0925 16:51:55.029584       1 shared_informer.go:320] Caches are synced for garbage collector
I0925 16:51:55.029626       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0925 16:52:27.217896       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="10.194791ms"
I0925 16:52:27.218123       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="130.528µs"


==> kube-proxy [185da108c4a2] <==
I0925 16:51:51.919262       1 server_linux.go:66] "Using iptables proxy"
I0925 16:51:52.097207       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0925 16:51:52.097262       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0925 16:51:52.116802       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0925 16:51:52.116857       1 server_linux.go:169] "Using iptables Proxier"
I0925 16:51:52.119252       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0925 16:51:52.119601       1 server.go:483] "Version info" version="v1.31.0"
I0925 16:51:52.119623       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 16:51:52.120559       1 config.go:197] "Starting service config controller"
I0925 16:51:52.120609       1 shared_informer.go:313] Waiting for caches to sync for service config
I0925 16:51:52.120630       1 config.go:326] "Starting node config controller"
I0925 16:51:52.120642       1 shared_informer.go:313] Waiting for caches to sync for node config
I0925 16:51:52.120642       1 config.go:104] "Starting endpoint slice config controller"
I0925 16:51:52.120671       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0925 16:51:52.220928       1 shared_informer.go:320] Caches are synced for node config
I0925 16:51:52.220966       1 shared_informer.go:320] Caches are synced for service config
I0925 16:51:52.220961       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [92ad83874791] <==
I0925 16:56:32.095131       1 server_linux.go:66] "Using iptables proxy"
I0925 16:56:32.272701       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0925 16:56:32.272751       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0925 16:56:32.293427       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0925 16:56:32.293489       1 server_linux.go:169] "Using iptables Proxier"
I0925 16:56:32.295897       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0925 16:56:32.296221       1 server.go:483] "Version info" version="v1.31.0"
I0925 16:56:32.296242       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 16:56:32.298148       1 config.go:104] "Starting endpoint slice config controller"
I0925 16:56:32.298168       1 config.go:197] "Starting service config controller"
I0925 16:56:32.298172       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0925 16:56:32.298187       1 shared_informer.go:313] Waiting for caches to sync for service config
I0925 16:56:32.298218       1 config.go:326] "Starting node config controller"
I0925 16:56:32.298231       1 shared_informer.go:313] Waiting for caches to sync for node config
I0925 16:56:32.398565       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0925 16:56:32.398623       1 shared_informer.go:320] Caches are synced for service config
I0925 16:56:32.398681       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [001c4ca6fe64] <==
I0925 16:56:28.752388       1 serving.go:386] Generated self-signed cert in-memory
W0925 16:56:30.438267       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0925 16:56:30.438445       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0925 16:56:30.438523       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0925 16:56:30.438903       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0925 16:56:30.489468       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0925 16:56:30.489531       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 16:56:30.491788       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0925 16:56:30.491858       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 16:56:30.491885       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0925 16:56:30.491961       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0925 16:56:30.593005       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [7bdcf16a3c9d] <==
I0925 16:51:49.445304       1 serving.go:386] Generated self-signed cert in-memory
W0925 16:51:50.968893       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0925 16:51:50.969379       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0925 16:51:50.969586       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0925 16:51:50.969760       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0925 16:51:51.005547       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0925 16:51:51.005577       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0925 16:51:51.007727       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0925 16:51:51.007766       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0925 16:51:51.007777       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 16:51:51.008005       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0925 16:51:51.108531       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0925 16:55:10.426991       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0925 16:55:10.427072       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0925 16:55:10.427318       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.094520    1477 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/c816a694-5377-4add-ad71-603f3f99b5a9-workspace-volume" (OuterVolumeSpecName: "workspace-volume") pod "c816a694-5377-4add-ad71-603f3f99b5a9" (UID: "c816a694-5377-4add-ad71-603f3f99b5a9"). InnerVolumeSpecName "workspace-volume". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.098226    1477 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c816a694-5377-4add-ad71-603f3f99b5a9-kube-api-access-6drbn" (OuterVolumeSpecName: "kube-api-access-6drbn") pod "c816a694-5377-4add-ad71-603f3f99b5a9" (UID: "c816a694-5377-4add-ad71-603f3f99b5a9"). InnerVolumeSpecName "kube-api-access-6drbn". PluginName "kubernetes.io/projected", VolumeGidValue ""
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.153177    1477 scope.go:117] "RemoveContainer" containerID="d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5"
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.175095    1477 scope.go:117] "RemoveContainer" containerID="67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f"
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.195160    1477 reconciler_common.go:288] "Volume detached for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/c816a694-5377-4add-ad71-603f3f99b5a9-workspace-volume\") on node \"minikube\" DevicePath \"\""
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.195221    1477 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-6drbn\" (UniqueName: \"kubernetes.io/projected/c816a694-5377-4add-ad71-603f3f99b5a9-kube-api-access-6drbn\") on node \"minikube\" DevicePath \"\""
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.199100    1477 scope.go:117] "RemoveContainer" containerID="d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5"
Sep 25 18:19:44 minikube kubelet[1477]: E0925 18:19:44.200578    1477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5" containerID="d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5"
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.200647    1477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5"} err="failed to get container status \"d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5\": rpc error: code = Unknown desc = Error response from daemon: No such container: d45c0f77c59a2bc1fdc54ea21ca446b67b40d931f78dbbc2bd2bb204a80df7b5"
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.200703    1477 scope.go:117] "RemoveContainer" containerID="67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f"
Sep 25 18:19:44 minikube kubelet[1477]: E0925 18:19:44.201998    1477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f" containerID="67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f"
Sep 25 18:19:44 minikube kubelet[1477]: I0925 18:19:44.202246    1477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f"} err="failed to get container status \"67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f\": rpc error: code = Unknown desc = Error response from daemon: No such container: 67d9a950c5327e09916cf49eb6fe6b57024b5ccb8ce99149f3341c71df4ead0f"
Sep 25 18:19:45 minikube kubelet[1477]: I0925 18:19:45.410600    1477 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="c816a694-5377-4add-ad71-603f3f99b5a9" path="/var/lib/kubelet/pods/c816a694-5377-4add-ad71-603f3f99b5a9/volumes"
Sep 25 18:19:50 minikube kubelet[1477]: E0925 18:19:50.925987    1477 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c816a694-5377-4add-ad71-603f3f99b5a9" containerName="docker"
Sep 25 18:19:50 minikube kubelet[1477]: E0925 18:19:50.926083    1477 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c816a694-5377-4add-ad71-603f3f99b5a9" containerName="jnlp"
Sep 25 18:19:50 minikube kubelet[1477]: I0925 18:19:50.926132    1477 memory_manager.go:354] "RemoveStaleState removing state" podUID="c816a694-5377-4add-ad71-603f3f99b5a9" containerName="docker"
Sep 25 18:19:50 minikube kubelet[1477]: I0925 18:19:50.926143    1477 memory_manager.go:354] "RemoveStaleState removing state" podUID="c816a694-5377-4add-ad71-603f3f99b5a9" containerName="jnlp"
Sep 25 18:19:51 minikube kubelet[1477]: I0925 18:19:51.058478    1477 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/c7a1599b-d6b5-488f-b19b-e08feb38d571-workspace-volume\") pod \"docker-pod-zfq43\" (UID: \"c7a1599b-d6b5-488f-b19b-e08feb38d571\") " pod="jenkins/docker-pod-zfq43"
Sep 25 18:19:51 minikube kubelet[1477]: I0925 18:19:51.058597    1477 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-g57nd\" (UniqueName: \"kubernetes.io/projected/c7a1599b-d6b5-488f-b19b-e08feb38d571-kube-api-access-g57nd\") pod \"docker-pod-zfq43\" (UID: \"c7a1599b-d6b5-488f-b19b-e08feb38d571\") " pod="jenkins/docker-pod-zfq43"
Sep 25 18:19:51 minikube kubelet[1477]: I0925 18:19:51.442782    1477 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f15aca1fc2d4fbe861be5a4f2a5a70f797a25fcfb5414ca8531588394bb1e058"
Sep 25 18:19:52 minikube kubelet[1477]: I0925 18:19:52.467967    1477 scope.go:117] "RemoveContainer" containerID="dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"
Sep 25 18:19:53 minikube kubelet[1477]: I0925 18:19:53.487557    1477 scope.go:117] "RemoveContainer" containerID="dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.497919    1477 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-g57nd\" (UniqueName: \"kubernetes.io/projected/c7a1599b-d6b5-488f-b19b-e08feb38d571-kube-api-access-g57nd\") pod \"c7a1599b-d6b5-488f-b19b-e08feb38d571\" (UID: \"c7a1599b-d6b5-488f-b19b-e08feb38d571\") "
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.498060    1477 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/c7a1599b-d6b5-488f-b19b-e08feb38d571-workspace-volume\") pod \"c7a1599b-d6b5-488f-b19b-e08feb38d571\" (UID: \"c7a1599b-d6b5-488f-b19b-e08feb38d571\") "
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.498852    1477 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/c7a1599b-d6b5-488f-b19b-e08feb38d571-workspace-volume" (OuterVolumeSpecName: "workspace-volume") pod "c7a1599b-d6b5-488f-b19b-e08feb38d571" (UID: "c7a1599b-d6b5-488f-b19b-e08feb38d571"). InnerVolumeSpecName "workspace-volume". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.502137    1477 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c7a1599b-d6b5-488f-b19b-e08feb38d571-kube-api-access-g57nd" (OuterVolumeSpecName: "kube-api-access-g57nd") pod "c7a1599b-d6b5-488f-b19b-e08feb38d571" (UID: "c7a1599b-d6b5-488f-b19b-e08feb38d571"). InnerVolumeSpecName "kube-api-access-g57nd". PluginName "kubernetes.io/projected", VolumeGidValue ""
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.530587    1477 scope.go:117] "RemoveContainer" containerID="aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.559141    1477 scope.go:117] "RemoveContainer" containerID="dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.580154    1477 scope.go:117] "RemoveContainer" containerID="aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d"
Sep 25 18:19:55 minikube kubelet[1477]: E0925 18:19:55.581384    1477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d" containerID="aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.581442    1477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d"} err="failed to get container status \"aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d\": rpc error: code = Unknown desc = Error response from daemon: No such container: aee6ce56237eff58dbc952baa624d668ef16f859dfa9c7e5bd72e1a0b8bda24d"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.581483    1477 scope.go:117] "RemoveContainer" containerID="dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"
Sep 25 18:19:55 minikube kubelet[1477]: E0925 18:19:55.582562    1477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce" containerID="dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.582637    1477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"} err="failed to get container status \"dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce\": rpc error: code = Unknown desc = Error response from daemon: No such container: dd2ae750fd80d79a8bd1eb06281f06346f0469cd83614a6d50f61dff1299ccce"
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.598992    1477 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-g57nd\" (UniqueName: \"kubernetes.io/projected/c7a1599b-d6b5-488f-b19b-e08feb38d571-kube-api-access-g57nd\") on node \"minikube\" DevicePath \"\""
Sep 25 18:19:55 minikube kubelet[1477]: I0925 18:19:55.599043    1477 reconciler_common.go:288] "Volume detached for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/c7a1599b-d6b5-488f-b19b-e08feb38d571-workspace-volume\") on node \"minikube\" DevicePath \"\""
Sep 25 18:19:57 minikube kubelet[1477]: I0925 18:19:57.408760    1477 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="c7a1599b-d6b5-488f-b19b-e08feb38d571" path="/var/lib/kubelet/pods/c7a1599b-d6b5-488f-b19b-e08feb38d571/volumes"
Sep 25 18:20:00 minikube kubelet[1477]: E0925 18:20:00.921426    1477 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c7a1599b-d6b5-488f-b19b-e08feb38d571" containerName="docker"
Sep 25 18:20:00 minikube kubelet[1477]: E0925 18:20:00.921489    1477 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c7a1599b-d6b5-488f-b19b-e08feb38d571" containerName="jnlp"
Sep 25 18:20:00 minikube kubelet[1477]: I0925 18:20:00.921563    1477 memory_manager.go:354] "RemoveStaleState removing state" podUID="c7a1599b-d6b5-488f-b19b-e08feb38d571" containerName="docker"
Sep 25 18:20:00 minikube kubelet[1477]: I0925 18:20:00.921590    1477 memory_manager.go:354] "RemoveStaleState removing state" podUID="c7a1599b-d6b5-488f-b19b-e08feb38d571" containerName="jnlp"
Sep 25 18:20:01 minikube kubelet[1477]: I0925 18:20:01.047006    1477 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/998419de-2626-473c-9d3f-484197547130-workspace-volume\") pod \"docker-pod-c92gn\" (UID: \"998419de-2626-473c-9d3f-484197547130\") " pod="jenkins/docker-pod-c92gn"
Sep 25 18:20:01 minikube kubelet[1477]: I0925 18:20:01.047082    1477 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bzql8\" (UniqueName: \"kubernetes.io/projected/998419de-2626-473c-9d3f-484197547130-kube-api-access-bzql8\") pod \"docker-pod-c92gn\" (UID: \"998419de-2626-473c-9d3f-484197547130\") " pod="jenkins/docker-pod-c92gn"
Sep 25 18:20:02 minikube kubelet[1477]: I0925 18:20:02.708470    1477 scope.go:117] "RemoveContainer" containerID="1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"
Sep 25 18:20:03 minikube kubelet[1477]: I0925 18:20:03.728656    1477 scope.go:117] "RemoveContainer" containerID="1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.680483    1477 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/998419de-2626-473c-9d3f-484197547130-workspace-volume\") pod \"998419de-2626-473c-9d3f-484197547130\" (UID: \"998419de-2626-473c-9d3f-484197547130\") "
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.680571    1477 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-bzql8\" (UniqueName: \"kubernetes.io/projected/998419de-2626-473c-9d3f-484197547130-kube-api-access-bzql8\") pod \"998419de-2626-473c-9d3f-484197547130\" (UID: \"998419de-2626-473c-9d3f-484197547130\") "
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.683788    1477 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/998419de-2626-473c-9d3f-484197547130-kube-api-access-bzql8" (OuterVolumeSpecName: "kube-api-access-bzql8") pod "998419de-2626-473c-9d3f-484197547130" (UID: "998419de-2626-473c-9d3f-484197547130"). InnerVolumeSpecName "kube-api-access-bzql8". PluginName "kubernetes.io/projected", VolumeGidValue ""
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.688856    1477 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/998419de-2626-473c-9d3f-484197547130-workspace-volume" (OuterVolumeSpecName: "workspace-volume") pod "998419de-2626-473c-9d3f-484197547130" (UID: "998419de-2626-473c-9d3f-484197547130"). InnerVolumeSpecName "workspace-volume". PluginName "kubernetes.io/empty-dir", VolumeGidValue ""
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.781651    1477 reconciler_common.go:288] "Volume detached for volume \"workspace-volume\" (UniqueName: \"kubernetes.io/empty-dir/998419de-2626-473c-9d3f-484197547130-workspace-volume\") on node \"minikube\" DevicePath \"\""
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.781695    1477 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-bzql8\" (UniqueName: \"kubernetes.io/projected/998419de-2626-473c-9d3f-484197547130-kube-api-access-bzql8\") on node \"minikube\" DevicePath \"\""
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.788450    1477 scope.go:117] "RemoveContainer" containerID="d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174"
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.814742    1477 scope.go:117] "RemoveContainer" containerID="1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.840993    1477 scope.go:117] "RemoveContainer" containerID="d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174"
Sep 25 18:20:05 minikube kubelet[1477]: E0925 18:20:05.842478    1477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174" containerID="d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174"
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.842539    1477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174"} err="failed to get container status \"d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174\": rpc error: code = Unknown desc = Error response from daemon: No such container: d0df8261de34a23cfc701b35eaa571d134cad18b3a9753bd386be147c7859174"
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.842580    1477 scope.go:117] "RemoveContainer" containerID="1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"
Sep 25 18:20:05 minikube kubelet[1477]: E0925 18:20:05.843961    1477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383" containerID="1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"
Sep 25 18:20:05 minikube kubelet[1477]: I0925 18:20:05.844020    1477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"} err="failed to get container status \"1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383\": rpc error: code = Unknown desc = Error response from daemon: No such container: 1f7246d837799f264453494da48c71690d7bdf9cfcb7c3058e7352185a00f383"
Sep 25 18:20:07 minikube kubelet[1477]: I0925 18:20:07.411839    1477 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="998419de-2626-473c-9d3f-484197547130" path="/var/lib/kubelet/pods/998419de-2626-473c-9d3f-484197547130/volumes"


==> storage-provisioner [b050f1277038] <==
I0925 16:57:17.519502       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0925 16:57:17.526844       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0925 16:57:17.526897       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0925 16:57:34.938201       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0925 16:57:34.938487       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"44f21a60-9d4f-4106-b643-ab524bf038f5", APIVersion:"v1", ResourceVersion:"20696", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_687c31b0-b4d8-4c90-8ed6-cdf3a58d9a12 became leader
I0925 16:57:34.938518       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_687c31b0-b4d8-4c90-8ed6-cdf3a58d9a12!
I0925 16:57:35.038833       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_687c31b0-b4d8-4c90-8ed6-cdf3a58d9a12!


==> storage-provisioner [bf42f0003288] <==
I0925 16:56:32.064837       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0925 16:57:02.068726       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

